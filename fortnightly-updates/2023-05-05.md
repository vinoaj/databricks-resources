# 2023-05-05

Hi team! I just wanted to share some of the interesting resources/articles that have come from Databricks in the last couple of weeks:

A lot of it is anchored around [Introducing Apache Spark‚Ñ¢ 3.4 for Databricks Runtime 13.0](https://www.databricks.com/blog/2023/04/14/introducing-apache-sparktm-34-databricks-runtime-130.html): *LOTS* of juicy announcements, and the ones that stood out for me:

- Spark Connect (more on that below üëá)
- Distributed training for PyTorch (more on that below üëá)
- Increased productivity: `DEFAULT` values for columns, lateral column alias references (i.e. easier to refer to derived columns within the same `SELECT` statement), `Dataset.to(schema)` easily convert a DataFrame's data types and column orders, parameterized SQL queries, `UNPIVOT`, `OFFSET` for paging, memory profiler for PySpark UDFs

## üß† ML & AI

- [PyTorch on Databricks - Introducing the Spark PyTorch Distributor](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html): You can now easily perform distributed PyTorch training with PySpark and Databricks! (sample [üìïNotebook](https://www.databricks.com/wp-content/uploads/notebooks/db-554/spark_pytorch_distributor_samples_v0.2.dbc)

- [Synthetic Data for Better Machine Learning](https://www.databricks.com/blog/2023/04/12/synthetic-data-better-machine-learning.html): a guide to using [Synthetic Data Vault (SDV)](https://github.com/sdv-dev/SDV) with MLflow to generate synthetic data, that reflects real-world data, for better models or safer data sharing between teams

- [Scale Vision Transformers (ViT) on the Databricks Lakehouse Platform with Spark NLP](https://www.databricks.com/blog/2023/04/19/scale-vision-transformers-vit-databricks-lakehouse-platform-spark-nlp.html): Spark NLP's latest release includes support for vision transformers. This guide walks through scaling ViTs on Databricks.

## üõ†Ô∏è Developer Experience

- [Spark Connect Available in Apache Spark 3.4: Run Spark Applications Everywhere](https://www.databricks.com/blog/2023/04/18/spark-connect-available-apache-spark.html): write PySpark in any environment and have your instructions processed in a remote Spark environment (e.g. Databricks cluster)! [`databricks-connect`](https://pypi.org/project/databricks-connect/) supports Spark Connect when using DBR13.0+. This simplifies client application development, mitigates memory contention on the Spark driver, separates dependency management for client applications, allows independent client and server upgrades, provides step-through IDE debugging, and thin client logging and metrics!
      ![Spark Connect](https://cms.databricks.com/sites/default/files/inline-images/db-571-blog-img-1.png)

- [Use Databricks from anywhere with Databricks Connect v2](https://www.databricks.com/blog/2023/04/18/use-databricks-anywhere-databricks-connect-v2.html): use the power of Databricks from any application running anywhere. It is also included in our [VS Code extension](https://marketplace.visualstudio.com/items?itemName=databricks.databricks) enabling built-in debugging of code on Databricks! Here's a [sample application](https://github.com/databricks-demos/dbconnect-plotly)
       ![Databricks Connect](https://cms.databricks.com/sites/default/files/inline-images/db-534-blog-img-1.png)

## ü™õ Data Engineering

- [How We Performed ETL on One Billion Records For Under $1 With Delta Live Tables (DLT)](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html): learn how to own a $1 terabyte scale incremental pipeline that includes varied data structures (CSV & XML), CDC with SCD Type II, modeling, and data quality enforcements, while all along utilising spot compute. Check out the DLT definitions in this [GitHub repo](https://github.com/shannon-barrow/databricks-tpc-di). What stood out for me were:
  - *2x speedup* due to DLT making efficient compute decisions
  - Data quality checks *exposed data errors others hadn't picked up before* (needle in a haystack of 1.5B records)
  - *Easy SCD Type II logic handling*

## üõí Retail

- [Retail in the Age of Generative AI 10 ways large language models (LLMs) may impact the retail industry](https://www.databricks.com/blog/2023/04/13/retail-age-generative-ai.html): our retail leaders' thoughts on where Generative AI can help retailers today

- [üìï Notebook: processing Salesforce Marketing Cloud email events and sending them to a CDP](https://d1r5llqwmkrl74.cloudfront.net/notebooks/RCG/amperity-sfmc-tracking/index.html#amperity-sfmc-tracking_1.html): demonstrates how SFMC email tracking extracts can be processed within Databricks, making the complete set available for detailed process analysis, and then condensed for consumption by a CDP

- [Developing an Open Data Ecosystem with SAP](https://www.databricks.com/blog/2023/04/19/developing-open-data-ecosystem-sap.html): *"Having this (operations/SAP) dataset in the lakehouse allows us to iterate in near real time with operations analysts, which is probably the primary value driver since the majority of our effort is spent aligning key stakeholders across operations, supply chain, and finance on a common set of KPIs"*

## ü•Ç Customer Stories

- [Introducing Atlassian Analytics](https://www.youtube.com/watch?v=7HRPJfjnnkw): [under NDA] Atlassian are leveraging the Databricks Lakehouse as the backend, and user queries are powered by Databricks SQL Pro

## Upcoming Events

- The [*Data+AI Summit* session catalog](https://register.dataaisummit.com/flow/db/dais2023/sessioncatalog23/page/sessioncatalog) is now live! Be sure to register now (free for virtual attendance) and you'll be able to access the recordings as soon as they're made available! 
- [Build Your Own Large Language Model Like Dolly](https://www.databricks.com/resources/webinar/apj-build-your-own-large-language-model-dolly) webinar on Apr 26th (today!) 2pm AEST
- [Enabling Production ML at Scale With Lakehouse](https://pages.databricks.com/202305-APJ-VE-ML-with-Lakehouse_LP---APJ-ML-Ops-Event-11-May.html) webinar on May 11 2pm AEST. Hear directly from our co-founder and VP of Engineering Patrick Wendell, Senior Director Product Craig Wiley (ex SageMaker & Vertex AI), Staff PM Kasey Uhlenhuth, and Barracuda Networks. You'll also receive a **50%** voucher for any Databricks certification exams!

As always, please let me know if you'd like to find out more about any of the announcements or use cases above üëÜüèΩ
