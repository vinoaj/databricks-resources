# 2023-04-12

## Upcoming Events

- The [**Data+AI Summit** session catalog](https://register.dataaisummit.com/flow/db/dais2023/sessioncatalog23/page/sessioncatalog) is now live! Be sure to register now (free for virtual attendance) and you'll be able to access the recordings as soon as they're made available! Please let me know if you plan to attend this live in San Francisco and I'll see if I can sort out some discount codes for you :)

- [Build Your Own Large Language Model Like Dolly](https://www.databricks.com/resources/webinar/apj-build-your-own-large-language-model-dolly) webinar on Apr 26th 2pm AEST

- [Enabling Production ML at Scale With Lakehouse](https://pages.databricks.com/202305-APJ-VE-ML-with-Lakehouse_LP---APJ-ML-Ops-Event-11-May.html) webinar on May 11 2pm AEST. Hear directly from our co-founder and VP of Engineering Patrick Wendell, Senior Director Product Craig Wiley (ex SageMaker & Vertex AI), Staff PM Kasey Uhlenhuth, and Barracuda Networks. You'll also receive a *50%* voucher for any Databricks certification exams!

## üõí Retail

- New **Solution Accelerator**! [Real-Time Propensity Estimation to Drive Online Sales](https://www.databricks.com/blog/2023/03/21/using-real-time-propensity-estimation-drive-online-sales.html): real-time scoring of purchase intent doesn't have to be hard! This Solution Accelerator walks you through the end-to-end process of having your own real-time scoring model on Databricks. Check out the [üìÑ detailed notebooks](https://d1r5llqwmkrl74.cloudfront.net/notebooks/RCG/clickstream-analytics/index.html#clickstream-analytics_1.html) that walk you through data preparation, ETL, model training with Feature Store, model registry, processing live events in streaming or batch, and deploying the model for real-time inference

- [Enhancing the Amperity CDP with Personalized Product Recommendations](https://www.databricks.com/blog/2023/03/15/enhancing-amperity-cdp-personalized-product-recommendations.html): move identity resolution data easily between Amperity and Databricks using [Amperity's¬†Databricks Delta table destination connector](https://docs.amperity.com/datagrid/destination_databricks_delta_table.html) (sample [üìï Notebook](https://d1r5llqwmkrl74.cloudfront.net/notebooks/RCG/amperity-cdp-rec/index.html#amperity-cdp-rec_1.html))

## üß† ML & AI

- [Dolly 2.0 was released!](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm): a 12B parameter language model fine-tuned on an instructional dataset crowdsourced among Databricks employees. What the community is most excited about is that we released our [`databricks-dolly-15k`](https://github.com/databrickslabs/dolly/tree/master/data) dataset (I'm glad to see all my submissions made it in there üòÅ) with an open licence including use in commercial applications! ([Hugging Face link](https://huggingface.co/databricks)). We wanted to showcase the ability for organisations to own their own LLMs trained on their own proprietary data without the need to share data with external parties. This is just the precursor to a number of exciting announcements that will be made at our [Data+AI Summit](https://register.dataaisummit.com/flow/db/dais2023/sessioncatalog23/page/sessioncatalog) in June.

- [Introducing AI Functions: Integrating Large Language Models with Databricks SQL](https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html): have you got a lot of unstructured data (e.g. customer feedback) sitting around and don't know what to do with it? Databricks' AI Functions allows you to easily apply OpenAI on your Lakehouse data through SQL functions. Using SQL only? And no setting up of complex API handlers? Yes please!

- [Introducing MLflow 2.3: Enhanced with Native LLM Support and New Features](https://www.databricks.com/blog/2023/04/18/introducing-mlflow-23-enhanced-native-llm-support-and-new-features.html): supporting seamless integration with Hugging Face Transformers, OpenAI functions, and LangChain. [Here's an example](https://dsmonk.medium.com/simplify-deep-discovery-using-a-i-for-your-videos-and-podcasts-using-databricks-langchain-661e22d10489) where my colleague used LangChain to analyse YouTube videos and register the resulting model in model registry for usage downstream

- [Unsupervised Outlier Detection on Databricks](https://www.databricks.com/blog/2023/03/13/unsupervised-outlier-detection-databricks.html) utilising Databricks' new [Kakapo package](https://pypi.org/project/databricks-kakapo/) (which integrates the vast [PyOD library](https://pyod.readthedocs.io/en/latest/) of outlier detection algorithms with MLFlow for tracking and packaging of models and [Hyperopt](http://hyperopt.github.io/hyperopt/) for exploring vast, complex and heterogeneous search spaces) (sample [üìï Notebook](https://d1r5llqwmkrl74.cloudfront.net/notebooks/PUB/rare-event-inspection/index.html))

## üîê Security

- [**New** Security best practices](https://www.databricks.com/blog/2023/03/30/security-best-practices-databricks-lakehouse-platform.html) including comprehensive checklists for [GCP](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), [Azure](https://www.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), and [AWS](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf)

- [GxP Best Practices Whitepaper](https://www.databricks.com/trust/gxp): GxP stands for "Good x Practices" and the variable `x` refers to a specific discipline, such as clinical, manufacturing or laboratory. The goal of GxP compliance is to ensure that regulated industries have a process that runs reliably, can survive failures and human error, and meets global traceability, accountability and data integrity requirements. No matter what industry you operate in, I believe these are solid practices to align with

- [An example](https://systemweakness.com/simplified-data-masking-in-databricks-8649adb3f60f) of using the [`MASK()`](https://docs.databricks.com/sql/language-manual/functions/mask.html) (available in DBR 12.2+) function for easy data masking

## üõ†Ô∏è Developer Experience

- [Run SQL Queries on Databricks From Visual Studio Code](https://www.databricks.com/blog/2023/03/29/run-sql-queries-databricks-visual-studio-code.html): makes life easy if you use the SQLTools extension and want to iterate on your SQL logic while in your local environment (e.g. while debugging `dbt` model logic) (download [Databricks driver for SQLTools](https://marketplace.visualstudio.com/items?itemName=databricks.sqltools-databricks-driver)

- [Exciting new updates coming to Workflows in April](https://www.databricks.com/blog/2023/04/04/exciting-new-updates-coming-workflows-april.html) including file arrival triggers, SQL file tasks, and UI improvements

- [‚ñ∂Ô∏è dbt Projects Integration in Databricks Workflows](https://www.youtube.com/watch?v=C4-FOpJzhKs)

## Delta Sharing

[Delta Sharing](https://docs.gcp.databricks.com/data-sharing/index.html) allows sharing of data between partners without data having to move or for the recipient to be a Databricks user. The imminent launch of [Databricks Marketplace](https://www.databricks.com/blog/2022/06/28/introducing-databricks-marketplace-an-open-marketplace-for-all-data-and-ai-assets.html) will provide a smooth vehicle for managing data partnerships

- [Using Delta Sharing to Accelerate Insights with Nasdaq‚Äôs Digital Assets Market Data](https://www.databricks.com/blog/2023/03/06/using-delta-sharing-accelerate-insights-nasdaqs-digital-assets-market-data.html): an example of analysing Nasdaq data shared via Delta Sharing (sample [üìï Notebooks](https://github.com/databricks-industry-solutions/nasdaq-crypto))

## ü•Ç Customer Stories

- [Having your cake and eating it too: How *Vizio* built a next-generation data platform to enable BI reporting, real-time streaming, and AI/ML](https://medium.com/@parveen.jindal/having-your-cake-and-eating-it-too-how-vizio-built-a-next-generation-data-platform-to-enable-bi-4fc42c539543): Vizio's journey in adopting the Lakehouse for a single platform that met their data warehouse and ML needs. *" Databricks was the only platform that could handle ETL, monitoring, orchestration, streaming, ML, and Data Governance on a single platform. Not only was Databricks SQL + Delta able to run queries faster on real-world data (in our analysis, Databricks was 3x faster) but we no longer needed to buy other services just to run the platform and add features in the future"*

- [**Ripple**: ML Training and Deployment Pipeline Using Databricks](https://engineering.ripple.com/ml-training-and-deployment-pipeline-using-databricks/): how Ripple uses Databricks to manage robust MLOps pipelines across a multi-cloud (GCP and AWS) architecture: *"ML flow tracking and MLflow API help coordinate these actions with ease in spite of using different platforms for model development, testing and deployment"*

## üéì Training & Education

I've been receiving a few questions around Databricks training & enablement. We have revamped our training options in the new year. Here's a cheat sheet of where to go for enablement

- [Free Live Onboarding Training](https://files.training.databricks.com/static/ilt-sessions/onboarding/index.html): no-cost, role-based onboarding training, multiple times a day across key geographic regions for Databricks customers, partners, as well as the general public
- [Customer Academy](https://customer-academy.databricks.com/learn): all your self-paced training and certification needs in one place
- [Paid Instructor-Lead Training (ILT) Training](https://www.databricks.com/learn/training/schedule)
- [Request private training](https://www.databricks.com/learn/training/private-training-requests-form)
- [Databricks Community](https://community.databricks.com/s/): community forum

As always, please let me know if you'd like to find out more about any of the announcements or use cases above üëÜüèΩ
